{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0522089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 The Qwen team, Alibaba Group. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Reference: https://platform.openai.com/docs/guides/function-calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c14d8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae5cf5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_agent.llm import get_chat_model\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit='fahrenheit'):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if 'tokyo' in location.lower():\n",
    "        return json.dumps({'location': 'Tokyo', 'temperature': '10', 'unit': 'celsius'})\n",
    "    elif 'san francisco' in location.lower():\n",
    "        return json.dumps({'location': 'San Francisco', 'temperature': '72', 'unit': 'fahrenheit'})\n",
    "    elif 'paris' in location.lower():\n",
    "        return json.dumps({'location': 'Paris', 'temperature': '22', 'unit': 'celsius'})\n",
    "    else:\n",
    "        return json.dumps({'location': location, 'temperature': 'unknown'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f14b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(fncall_prompt_type: str = 'qwen'):\n",
    "    llm = get_chat_model({\n",
    "        # Use the model service provided by DashScope:\n",
    "        'model': 'qwen-plus-latest',\n",
    "        'model_server': 'dashscope',\n",
    "        'api_key': os.getenv('DASHSCOPE_API_KEY'),\n",
    "        'generate_cfg': {\n",
    "            'fncall_prompt_type': fncall_prompt_type\n",
    "        },\n",
    "\n",
    "        # Use the OpenAI-compatible model service provided by DashScope:\n",
    "        # 'model': 'qwen2.5-72b-instruct',\n",
    "        # 'model_server': 'https://dashscope.aliyuncs.com/compatible-mode/v1',\n",
    "        # 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n",
    "\n",
    "        # Use the model service provided by Together.AI:\n",
    "        # 'model': 'Qwen/qwen2.5-7b-instruct',\n",
    "        # 'model_server': 'https://api.together.xyz',  # api_base\n",
    "        # 'api_key': os.getenv('TOGETHER_API_KEY'),\n",
    "\n",
    "        # Use your own model service compatible with OpenAI API:\n",
    "        # 'model': 'Qwen/qwen2.5-7b-instruct',\n",
    "        # 'model_server': 'http://localhost:8000/v1',  # api_base\n",
    "        # 'api_key': 'EMPTY',\n",
    "    })\n",
    "\n",
    "    # Step 1: send the conversation and available functions to the model\n",
    "    messages = [{'role': 'user', 'content': \"What's the weather like in San Francisco?\"}]\n",
    "    functions = [{\n",
    "        'name': 'get_current_weather',\n",
    "        'description': 'Get the current weather in a given location',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'location': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'The city and state, e.g. San Francisco, CA',\n",
    "                },\n",
    "                'unit': {\n",
    "                    'type': 'string',\n",
    "                    'enum': ['celsius', 'fahrenheit']\n",
    "                },\n",
    "            },\n",
    "            'required': ['location'],\n",
    "        },\n",
    "    }]\n",
    "\n",
    "    print('# Assistant Response 1:')\n",
    "    responses = []\n",
    "    for responses in llm.chat(\n",
    "            messages=messages,\n",
    "            functions=functions,\n",
    "            stream=True,\n",
    "            # Note: extra_generate_cfg is optional\n",
    "            # extra_generate_cfg=dict(\n",
    "            #     # Note: if function_choice='auto', let the model decide whether to call a function or not\n",
    "            #     # function_choice='auto',  # 'auto' is the default if function_choice is not set\n",
    "            #     # Note: set function_choice='get_current_weather' to force the model to call this function\n",
    "            #     function_choice='get_current_weather',\n",
    "            # ),\n",
    "    ):\n",
    "        print(responses)\n",
    "\n",
    "    # If you do not need streaming output, you can either use the following trick:\n",
    "    #   *_, responses = llm.chat(messages=messages, functions=functions, stream=True)\n",
    "    # or use stream=False:\n",
    "    #   responses = llm.chat(messages=messages, functions=functions, stream=False)\n",
    "\n",
    "    messages.extend(responses)  # extend conversation with assistant's reply\n",
    "\n",
    "    # Step 2: check if the model wanted to call a function\n",
    "    last_response = messages[-1]\n",
    "    if last_response.get('function_call', None):\n",
    "\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            'get_current_weather': get_current_weather,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        function_name = last_response['function_call']['name']\n",
    "        function_to_call = available_functions[function_name]\n",
    "        function_args = json.loads(last_response['function_call']['arguments'])\n",
    "        function_response = function_to_call(\n",
    "            location=function_args.get('location'),\n",
    "            unit=function_args.get('unit'),\n",
    "        )\n",
    "        print('# Function Response:')\n",
    "        print(function_response)\n",
    "\n",
    "        # Step 4: send the info for each function call and function response to the model\n",
    "        messages.append({\n",
    "            'role': 'function',\n",
    "            'name': function_name,\n",
    "            'content': function_response,\n",
    "        })  # extend conversation with function response\n",
    "\n",
    "        print('# Assistant Response 2:')\n",
    "        for responses in llm.chat(\n",
    "                messages=messages,\n",
    "                functions=functions,\n",
    "                stream=True,\n",
    "        ):  # get a new response from the model where it can see the function response\n",
    "            print(responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc42a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Assistant Response 1:\n",
      "[{'role': 'assistant', 'content': '', 'function_call': {'name': 'get_current_weather', 'arguments': ''}, 'extra': {'model_service_info': {'status_code': <HTTPStatus.OK: 200>, 'request_id': 'df132152-4208-9b08-b8d9-3d802412b176', 'code': '', 'message': '', 'output': {'text': None, 'choices': [{'finish_reason': 'null', 'message': {}}], 'finish_reason': None}, 'usage': {'input_tokens': 200, 'output_tokens': 12}}}}]\n",
      "[{'role': 'assistant', 'content': '', 'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\"'}, 'extra': {'model_service_info': {'status_code': <HTTPStatus.OK: 200>, 'request_id': 'df132152-4208-9b08-b8d9-3d802412b176', 'code': '', 'message': '', 'output': {'text': None, 'choices': [{'finish_reason': 'null', 'message': {}}], 'finish_reason': None}, 'usage': {'input_tokens': 200, 'output_tokens': 16}}}}]\n",
      "[{'role': 'assistant', 'content': '', 'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"San Francisco'}, 'extra': {'model_service_info': {'status_code': <HTTPStatus.OK: 200>, 'request_id': 'df132152-4208-9b08-b8d9-3d802412b176', 'code': '', 'message': '', 'output': {'text': None, 'choices': [{'finish_reason': 'null', 'message': {}}], 'finish_reason': None}, 'usage': {'input_tokens': 200, 'output_tokens': 20}}}}]\n",
      "[{'role': 'assistant', 'content': '', 'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"San Francisco, CA\"}'}, 'extra': {'model_service_info': {'status_code': <HTTPStatus.OK: 200>, 'request_id': 'df132152-4208-9b08-b8d9-3d802412b176', 'code': '', 'message': '', 'output': {'text': None, 'choices': [{'finish_reason': 'null', 'message': {}}], 'finish_reason': None}, 'usage': {'input_tokens': 200, 'output_tokens': 23}}}}]\n",
      "[{'role': 'assistant', 'content': '', 'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"San Francisco, CA\"}'}, 'extra': {'model_service_info': {'status_code': <HTTPStatus.OK: 200>, 'request_id': 'df132152-4208-9b08-b8d9-3d802412b176', 'code': '', 'message': '', 'output': {'text': None, 'choices': [{'finish_reason': 'stop', 'message': {}}], 'finish_reason': None}, 'usage': {'input_tokens': 200, 'output_tokens': 23}}}}]\n",
      "# Function Response:\n",
      "{\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"}\n",
      "# Assistant Response 2:\n",
      "[{'role': 'assistant', 'content': 'The', 'extra': {'model_service_info': {'status_code': <HTTPStatus.OK: 200>, 'request_id': '549df0da-b16e-95e7-af99-c69ee0c660c9', 'code': '', 'message': '', 'output': {'text': None, 'choices': [{'finish_reason': 'null', 'message': {}}], 'finish_reason': None}, 'usage': {'input_tokens': 257, 'output_tokens': 1}}}}]\n",
      "[{'role': 'assistant', 'content': 'The current', 'extra': {'model_service_info': {'status_code': <HTTPStatus.OK: 200>, 'request_id': '549df0da-b16e-95e7-af99-c69ee0c660c9', 'code': '', 'message': '', 'output': {'text': None, 'choices': [{'finish_reason': 'null', 'message': {}}], 'finish_reason': None}, 'usage': {'input_tokens': 257, 'output_tokens': 2}}}}]\n",
      "[{'role': 'assistant', 'content': 'The current weather', 'extra': {'model_service_info': {'status_code': <HTTPStatus.OK: 200>, 'request_id': '549df0da-b16e-95e7-af99-c69ee0c660c9', 'code': '', 'message': '', 'output': {'text': None, 'choices': [{'finish_reason': 'null', 'message': {}}], 'finish_reason': None}, 'usage': {'input_tokens': 257, 'output_tokens': 3}}}}]\n",
      "[{'role': 'assistant', 'content': 'The current weather in', 'extra': {'model_service_info': {'status_code': <HTTPStatus.OK: 200>, 'request_id': '549df0da-b16e-95e7-af99-c69ee0c660c9', 'code': '', 'message': '', 'output': {'text': None, 'choices': [{'finish_reason': 'null', 'message': {}}], 'finish_reason': None}, 'usage': {'input_tokens': 257, 'output_tokens': 4}}}}]\n",
      "[{'role': 'assistant', 'content': 'The current weather in San Francisco is ', 'extra': {'model_service_info': {'status_code': <HTTPStatus.OK: 200>, 'request_id': '549df0da-b16e-95e7-af99-c69ee0c660c9', 'code': '', 'message': '', 'output': {'text': None, 'choices': [{'finish_reason': 'null', 'message': {}}], 'finish_reason': None}, 'usage': {'input_tokens': 257, 'output_tokens': 8}}}}]\n",
      "[{'role': 'assistant', 'content': 'The current weather in San Francisco is 72°F.', 'extra': {'model_service_info': {'status_code': <HTTPStatus.OK: 200>, 'request_id': '549df0da-b16e-95e7-af99-c69ee0c660c9', 'code': '', 'message': '', 'output': {'text': None, 'choices': [{'finish_reason': 'null', 'message': {}}], 'finish_reason': None}, 'usage': {'input_tokens': 257, 'output_tokens': 12}}}}]\n",
      "[{'role': 'assistant', 'content': 'The current weather in San Francisco is 72°F.', 'extra': {'model_service_info': {'status_code': <HTTPStatus.OK: 200>, 'request_id': '549df0da-b16e-95e7-af99-c69ee0c660c9', 'code': '', 'message': '', 'output': {'text': None, 'choices': [{'finish_reason': 'stop', 'message': {}}], 'finish_reason': None}, 'usage': {'input_tokens': 257, 'output_tokens': 12}}}}]\n"
     ]
    }
   ],
   "source": [
    "# Run example of function calling with QwenFnCallPrompt\n",
    "# test(fncall_prompt_type='qwen')\n",
    "\n",
    "# Run example of function calling with NousFnCallPrompt\n",
    "test(fncall_prompt_type='nous')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
